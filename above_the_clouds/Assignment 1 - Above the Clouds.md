
This paper covers the state of the Cloud Computing industry at the time it was published (2009), and tries to establish common definitions for talking about it.  While I think the definitions are well done and stand up very well to this day, getting any two people in our industry to agree on the definition of a word is an uphill battle, especially once marketing gets involved.

I enjoyed seeing some of the predictions and statistics from 15 years ago; many of the gaps identified in the paper have been addressed.  I laughed out loud when I read "In less than two year, Amazon Web Services increased the number of different types of compute servers from one to five, and in less than one year they added seven new infrastructure services...."  They certainly did not stop there!  As of today, they offer 695 instance types (in us-east-1) and 194 distinct services.  Many of these new services address limitations called out in the paper.  For example, the "FedExing Disks" method (the successor to the ever-popular "station wagon full of tapes" model) has been adopted by AWS in the form of their Snowcone, Snowball, and Snowmobile services, effectively addressing data transfer bottle necks for customers with large transfer needs and correspondingly deep pockets.

Other obstacles have been addressed by the wider community.  For instance, debugging large scale distributed systems is still a major challenge, but there are some excellent tools appearing to help.  A Service Mesh (like the open source Istio, Linkerd, and Cillium) can go a long way toward simplifying the management of a distributed system.  And sampling tracing systems like OpenTelemetry make it possible to follow requests through any number of services.

"Computing as a Utility" is closer than ever, but its full implementation may remain a dream because here the interests of Cloud Users and Cloud Providers diverge.  Users of the Cloud would benefit from standardized APIs for basic compute, storage, and networking services that all providers have.  However, the providers depend too much on lock-in to keep their profit margins as high as they are accustomed to.  They would not support any effort that would reduce these services to commodities and be forced to compete on price, quality, or other criteria.  So, while there are tools like Terraform that can manage basic resources from any Cloud provider, the APIs for these services remain, and in my opinion will always remain, incompatible.

From a software engineering perspective, the best potential for standardization I can see is Kubernetes, because it defines a standard API, and is too popular for providers to ignore.  There is still a long way to go for Kubernetes adoption, because many users see it as too complicated for their needs, but there is plenty of room for higher level APIs built on top of the Kubernetes API, and they will automatically be supported everywhere.  If adoption of Kubernetes as a standard layer takes off, the dream of utility computing may be within reach.  We will be able to deploy standardized workloads onto any public or private cloud that implements the Kubernetes API, without having to know or care about the details of the underlying infrastructure, and easily move our workloads around as needs change.

On the other hand, data lock-in is still very real.  Take for example AWS's "Roach Motel"-inspired pricing for data transfer.  Transfer into most AWS services is free, but the costs for transferring data out add up very quickly.  Even if it were easy to move a workload to another public or private cloud, the time and expense of a data migration are significant.  This serves providers' interests, so I do not see a reason that it would change in the future.  CloudFlare's R2 Storage service is interesting in this regard because they do not charge for outgoing bandwidth.   Perhaps if this service siphons away enough business from the larger Cloud providers, it may disrupt the business model. 